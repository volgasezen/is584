{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ee5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install levenshtein\n",
    "!pip install snowballstemmer\n",
    "!pip install zeyrek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fdf8fc",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f460e",
   "metadata": {},
   "source": [
    "When we need to modify text files before further processing, we could utilize python's string default string methods. These methods allow us to do some simple operations on our input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1bf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loo', '_here']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dum_text = \"look_here\"\n",
    "dum_text.split(r\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552894d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'look_here'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dum_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d4a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dum_text.find(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87590c",
   "metadata": {},
   "source": [
    "Although Python's default operations help us in essential modifications, we need to utilize regular expressions. For this purpose, we will use the `re` library to use our regular expression patterns on an input text. `re` library gives different options to utilize regular expressions like search(), findall(), sub(), and split(). Let's check a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc81ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='find'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "print(re.match(\"my\",\"findmyphone\"))\n",
    "re.match(\"find\",\"findmyphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11952fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'number 6r'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.search(r'n.*r', 'number 6r').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59aba75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3', '6']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.findall(r'[0-9]','2 times 3 is equalto 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629328dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This tree is big.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.sub(\"small\",\"big\",'This tree is small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b70e26",
   "metadata": {},
   "source": [
    "> The company's database is critical to their business operations. They have a dedicated team that ensures the accuracy and security of the database. However, there was an issue that caused the loss of some important data. The team is currently working to recover the lost ***data*** and improve the overall database system.\n",
    "\n",
    "Considering the above text, Create a regex that will match only the bold word .\n",
    "\n",
    "Tip: use `findall` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75023b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'data', 'data', 'data', 'data']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(14, 18), match='data'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = '''The company's database is critical to their business operations.\n",
    "          They have a dedicated team that ensures the accuracy and security\n",
    "          of the database. However, there was an issue that caused the loss\n",
    "          of some important data. The team is currently working to recover\n",
    "          the lost data and improve the overall database system.'''\n",
    "\n",
    "myregex = r\"data\"\n",
    "print(re.findall(myregex,text))\n",
    "re.search(myregex,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' data ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(270, 276), match=' data '>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# myregex = ## FILL IN HERE ##\n",
    "print(re.findall(myregex,text))\n",
    "re.search(myregex,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f428d2",
   "metadata": {},
   "source": [
    ">\"The function takes two arguments (x and y) and returns their sum. The output is then printed to the console using the print() function. The parentheses ensure that the arguments are passed to the function correctly and that the output is displayed as intended.\"\n",
    "\n",
    "Considering the above text, Create a regex that will get the text between parentheses ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x and y', \"'asd'\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = '''The function takes two arguments (x and y) and returns their sum.\n",
    "        The output is then printed to the console using the print('asd') \n",
    "        function. The parentheses ensure that the arguments are passed to \n",
    "        the function correctly and that the output is displayed as intended.'''\n",
    "\n",
    "# myregex = ## FILL IN HERE ##\n",
    "re.findall(myregex,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057b0b7",
   "metadata": {},
   "source": [
    "### Discussion Question\n",
    "\n",
    "* What other use cases are there for regex?\n",
    "\n",
    "* Come up with a simple rule of your own (psuedo-code is fine) for sentiment analysis.<br> (Figuring out how positive or negative the emotion expressed in the sentence is.)<br> What would be the advantages and risks of using such rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec6618",
   "metadata": {},
   "source": [
    "> Answers can be written here by double clicking and editing the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b510b7a",
   "metadata": {},
   "source": [
    "## Part-of-speech<a id=\"pos\"></a>\n",
    "\n",
    "Before we move along, we should take a look at part-of-speech (commonly referred as \"POS\") tagging. Sometimes we need to classify words according to their function (part-of-speech) in the sentence, so that we can extract certain information. For example, if we need to analyze verbs in a long text, we can use words' POS tags and filter out words that are not verbs, which would significantly simplify the process. \n",
    "\n",
    "POS tags are especially useful when a word can have different functions in a sentence with the exact same form, so we cannot just take a look at the word itself and draw conclusions. For example, \"type\" can mean a category or a verb (to type). For these reasons, we have POS taggers. A POS tagger classifies each unit's syntactic function in the sentence. There are different types of POS taggers. The one we will use is actually a pre-trained machine learning classifier of NLTK. The perceptron model is trained with a [treebank](https://en.wikipedia.org/wiki/Treebank) (a corpus with annotated POS tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 'WDT'), ('type', 'NN'), ('of', 'IN'), ('typewriter', 'NN'), ('would', 'MD'), ('you', 'PRP'), ('like', 'VB'), ('to', 'TO'), ('type', 'VB'), ('with', 'IN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"Which type of typewriter would you like to type with?\"\n",
    "\n",
    "# Uppercase letters can confuse POS tagging, so we need to lowercase everything. This\n",
    "# is automatically handled by our tokenizer anyway. Note that a truecasing approach or\n",
    "# a more simplified approach such as only touching the first letter of a sentence could\n",
    "# potentially yield better results in POS tagging. You can check\n",
    "# https://en.wikipedia.org/wiki/Truecasing and\n",
    "# https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21\n",
    "# to read more about truecasing.\n",
    "sentence_tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# POS tagging:\n",
    "sentence_tokens_pos = pos_tag(sentence_tokens)\n",
    "\n",
    "print(sentence_tokens_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c3450",
   "metadata": {},
   "source": [
    "As you can see, each term is now classified. However, the tags are not very clear for us. We can check the documentation or use a dictionary to read the explanation and see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: which \n",
      "POS tag: WDT \n",
      "Explanation: WH-determiner \n",
      "Example: that what whatever which whichever  \n",
      "\n",
      "Token: type \n",
      "POS tag: NN \n",
      "Explanation: noun, common, singular or mass \n",
      "Example: common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ...  \n",
      "\n",
      "Token: of \n",
      "POS tag: IN \n",
      "Explanation: preposition or conjunction, subordinating \n",
      "Example: astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ...  \n",
      "\n",
      "Token: typewriter \n",
      "POS tag: NN \n",
      "Explanation: noun, common, singular or mass \n",
      "Example: common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ...  \n",
      "\n",
      "Token: would \n",
      "POS tag: MD \n",
      "Explanation: modal auxiliary \n",
      "Example: can cannot could couldn't dare may might must need ought shall should shouldn't will would  \n",
      "\n",
      "Token: you \n",
      "POS tag: PRP \n",
      "Explanation: pronoun, personal \n",
      "Example: hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us  \n",
      "\n",
      "Token: like \n",
      "POS tag: VB \n",
      "Explanation: verb, base form \n",
      "Example: ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ...  \n",
      "\n",
      "Token: to \n",
      "POS tag: TO \n",
      "Explanation: \"to\" as preposition or infinitive marker \n",
      "Example: to  \n",
      "\n",
      "Token: type \n",
      "POS tag: VB \n",
      "Explanation: verb, base form \n",
      "Example: ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ...  \n",
      "\n",
      "Token: with \n",
      "POS tag: IN \n",
      "Explanation: preposition or conjunction, subordinating \n",
      "Example: astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ...  \n",
      "\n",
      "Token: ? \n",
      "POS tag: . \n",
      "Explanation: sentence terminator \n",
      "Example: . ! ?  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tag_dict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "for token in sentence_tokens_pos:\n",
    "    print(\"Token:\",token[0],\"\\nPOS tag:\",token[1],\"\\nExplanation:\",tag_dict[token[1]][0],\"\\nExample:\",tag_dict[token[1]][1],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5fae1",
   "metadata": {},
   "source": [
    "See that the first \"type\" is classified as a noun (\"NN\") while the last one is classified as a verb (\"VB\"). It is not bad for general purposes. This will come handy later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c25138",
   "metadata": {},
   "source": [
    "### Discussion Question\n",
    "\n",
    "* Can a universal POS tagging algorithm be created? (Consider English versus Turkish.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b9d8a",
   "metadata": {},
   "source": [
    "## Levenshtein distance<a id=\"levenshtein\"></a>\n",
    "\n",
    "As mentioned above, a simple autocorrection process can be applied using the Levenshtein distance. Let us see what that means.\n",
    "\n",
    "Consider the words `cup` and `cap`. Their Levenshtein distance is 1, because we can obtain one from the other by simply substituting a character. For two given strings, we can calculate the number of operations required (edit distance) to obtain one from the another. These operations are:\n",
    "\n",
    "* Insertion: Inserting a character to a specific location in the string.\n",
    "    * \"up\" becomes \"**c**up\"\n",
    "* Deletion: Deleting a character from a specific location in the string.\n",
    "    * \"cu**s**p\" becomes \"cup\"\n",
    "* Substitution: Substituting a character in a specific location in the string with another character.\n",
    "    * \"c**u**p\" becomes \"c**a**p\"\n",
    "* Transpotisition (this is later introduced by an extension, the Damerau-Levenstein distance algorithm): Switching the positions of two adjacent characters.\n",
    "    * \"c**pu**\" becomes \"c**up**\"\n",
    "    \n",
    "Using these, for a given word, we can find the closest word from a dictionary that would require the least amount of changes. This is a costly process, so it is usually limited to certain amount of changes. See [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) for more information.\n",
    "\n",
    "We can also use this distance to calculate the similarity between two strings. This is handy for fuzzy string matching, when the same thing can be represented in similar yet different forms. This is quite common in neighborhood or street names in Turkey. By setting a similarity threshold and looking at their similarity, we can match addresses like `Kemalpaşa Mah.` and `Kemal Paşa Mahallesi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc31d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "string_a = \"Kemalpaşa Mah.\"\n",
    "string_b = \"Kemal Paşa Mahallesi\"\n",
    "\n",
    "Levenshtein.ratio(string_a, string_b)\n",
    "# Note that you would probably want to remove \"mahallesi\" or \"mah.\" when your task is\n",
    "# address matching. It would significantly increase your success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f705f3",
   "metadata": {},
   "source": [
    "A use case for our dataset could be finding tweets that are similar to each other. Let us search for a tweet pair that has the highest similarity without being exactly the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c81637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar tweet pair: ('395 new cases and 3 new deaths in Uzbekistan [13:22 GMT] #coronavirus #CoronaVirusUpdate #COVID19 #CoronavirusPandemic', '1,005 new cases and 18 new deaths in the United States [13:16 GMT] #coronavirus #CoronaVirusUpdate #COVID19 #CoronavirusPandemic')\n",
      "Similarity 0.8699186991869918\n"
     ]
    }
   ],
   "source": [
    "pair = None\n",
    "highest = 0\n",
    "\n",
    "tweets_to_compare = 100\n",
    "# To compare all the tweets, uncomment this line. Note that it would take much longer.\n",
    "# tweets_to_compare = dataset.shape[0]\n",
    "\n",
    "# This compares each tweet with the ones that come after itself, which takes some time.\n",
    "for i in range(0, tweets_to_compare-1):\n",
    "    for j in range(1, tweets_to_compare):\n",
    "        # If the tweets are not the same:\n",
    "        if dataset.loc[i,\"text\"] != dataset.loc[j,\"text\"]:\n",
    "            similarity = Levenshtein.ratio(dataset.loc[i,\"text\"], dataset.loc[j,\"text\"])\n",
    "            # If their similarity is higher than the previous similarities:\n",
    "            if similarity > highest:\n",
    "                highest = similarity\n",
    "                pair = (dataset.loc[i,\"text\"], dataset.loc[j,\"text\"])\n",
    "\n",
    "print(f\"The most similar tweet pair: {pair}\")\n",
    "print(f\"Similarity: {highest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefec85c",
   "metadata": {},
   "source": [
    "## Bonus: NLP in Turkish<a id=\"tr\"></a>\n",
    "\n",
    "From a linguistic perspective, Turkish is a fascinating language with its rather strict grammatical rules. However, due to its agglutinative nature, words can easily become complex with many affixes and inflections. This can make morphological analyses harder compared to English.\n",
    "\n",
    "### Stemming<a id=\"tr-stem\"></a>\n",
    "\n",
    "[snowballstemmer](https://pypi.org/project/snowballstemmer/) has a Turkish stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d578bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gözle', 'gözleye', 'gözlüklü', 'gözcü', 'göz', 'düş', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "stemmer_tr = TurkishStemmer()\n",
    "\n",
    "sentence = \"Gözleme gözleyen gözlüklü gözcü gözden düştü.\"\n",
    "\n",
    "[stemmer_tr.stemWord(token) for token in tokenizer.tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ef4fe",
   "metadata": {},
   "source": [
    "### POS tagging and lemmatization<a id=\"tr-pos-lemma\"></a>\n",
    "\n",
    "It looks like the morphological analyzer of [Zemberek](https://github.com/ahmetaa/zemberek-nlp), the famous Turkish NLP tool for Java, has been unofficially ported to Python as [zeyrek](https://github.com/obulat/zeyrek/). It does not have all of its original features (like disambiguation and more), but we can still use it to morphologically analyze a word, sentence, or sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66f0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"# analyzer\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"G\\u00f6zleme\",\n          \"g\\u00f6zleyen\",\n          \"d\\u00fc\\u015ft\\u00fc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"d\\u00fc\\u015f\",\n          \"d\\u00fc\\u015fmek\",\n          \"g\\u00f6zlemek\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Noun\",\n          \"Punc\",\n          \"Verb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"formatted\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"[g\\u00f6zlemek:Verb] g\\u00f6zle:Verb+me:Neg+Imp+A2sg\",\n          \"[g\\u00f6zlem:Noun] g\\u00f6zlem:Noun+A3sg+e:Dat\",\n          \"[g\\u00f6zlemek:Verb] g\\u00f6zle:Verb|yen:PresPart\\u2192Adj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-7aa1fabc-88cb-4632-a048-e2fdaff749aa\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gözleme</td>\n",
       "      <td>gözlemek</td>\n",
       "      <td>Verb</td>\n",
       "      <td>[gözlemek:Verb] gözle:Verb+me:Neg+Imp+A2sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gözleme</td>\n",
       "      <td>gözlem</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[gözlem:Noun] gözlem:Noun+A3sg+e:Dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gözleme</td>\n",
       "      <td>Gözlem</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[Gözlem:Noun,Prop] gözlem:Noun+A3sg+e:Dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gözleme</td>\n",
       "      <td>gözleme</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[gözleme:Noun] gözleme:Noun+A3sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gözleme</td>\n",
       "      <td>gözlemek</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[gözlemek:Verb] gözle:Verb|me:Inf2→Noun+A3sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gözleyen</td>\n",
       "      <td>gözlemek</td>\n",
       "      <td>Adj</td>\n",
       "      <td>[gözlemek:Verb] gözle:Verb|yen:PresPart→Adj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gözlüklü</td>\n",
       "      <td>gözlük</td>\n",
       "      <td>Adj</td>\n",
       "      <td>[gözlük:Noun] gözlük:Noun+A3sg|lü:With→Adj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gözlüklü</td>\n",
       "      <td>göz</td>\n",
       "      <td>Adj</td>\n",
       "      <td>[göz:Noun] göz:Noun+A3sg|lük:Ness→Noun+A3sg|lü:With→Adj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gözcü</td>\n",
       "      <td>gözcü</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[gözcü:Noun] gözcü:Noun+A3sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gözcü</td>\n",
       "      <td>göz</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[göz:Noun] göz:Noun+A3sg|cü:Agt→Noun+A3sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gözden</td>\n",
       "      <td>göz</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[göz:Noun] göz:Noun+A3sg+den:Abl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gözden</td>\n",
       "      <td>Gözde</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[Gözde:Noun,Prop] gözde:Noun+A3sg+n:P2sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gözden</td>\n",
       "      <td>gözde</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[gözde:Noun] gözde:Noun+A3sg+n:P2sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gözden</td>\n",
       "      <td>gözde</td>\n",
       "      <td>Noun</td>\n",
       "      <td>[gözde:Adj] gözde:Adj|Zero→Noun+A3sg+n:P2sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>düştü</td>\n",
       "      <td>düşmek</td>\n",
       "      <td>Verb</td>\n",
       "      <td>[düşmek:Verb] düş:Verb+tü:Past+A3sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>düştü</td>\n",
       "      <td>düş</td>\n",
       "      <td>Verb</td>\n",
       "      <td>[düş:Noun] düş:Noun+A3sg|Zero→Verb+tü:Past+A3sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>Punc</td>\n",
       "      <td>[.:Punc] .:Punc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7aa1fabc-88cb-4632-a048-e2fdaff749aa')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7aa1fabc-88cb-4632-a048-e2fdaff749aa button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7aa1fabc-88cb-4632-a048-e2fdaff749aa');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-3fe052ac-529e-4e15-861e-967f5e150641\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fe052ac-529e-4e15-861e-967f5e150641')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-3fe052ac-529e-4e15-861e-967f5e150641 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        word     lemma   pos  \\\n",
       "0   Gözleme   gözlemek  Verb   \n",
       "1   Gözleme   gözlem    Noun   \n",
       "2   Gözleme   Gözlem    Noun   \n",
       "3   Gözleme   gözleme   Noun   \n",
       "4   Gözleme   gözlemek  Noun   \n",
       "5   gözleyen  gözlemek  Adj    \n",
       "6   gözlüklü  gözlük    Adj    \n",
       "7   gözlüklü  göz       Adj    \n",
       "8   gözcü     gözcü     Noun   \n",
       "9   gözcü     göz       Noun   \n",
       "10  gözden    göz       Noun   \n",
       "11  gözden    Gözde     Noun   \n",
       "12  gözden    gözde     Noun   \n",
       "13  gözden    gözde     Noun   \n",
       "14  düştü     düşmek    Verb   \n",
       "15  düştü     düş       Verb   \n",
       "16  .         .         Punc   \n",
       "\n",
       "                                                  formatted  \n",
       "0   [gözlemek:Verb] gözle:Verb+me:Neg+Imp+A2sg               \n",
       "1   [gözlem:Noun] gözlem:Noun+A3sg+e:Dat                     \n",
       "2   [Gözlem:Noun,Prop] gözlem:Noun+A3sg+e:Dat                \n",
       "3   [gözleme:Noun] gözleme:Noun+A3sg                         \n",
       "4   [gözlemek:Verb] gözle:Verb|me:Inf2→Noun+A3sg             \n",
       "5   [gözlemek:Verb] gözle:Verb|yen:PresPart→Adj              \n",
       "6   [gözlük:Noun] gözlük:Noun+A3sg|lü:With→Adj               \n",
       "7   [göz:Noun] göz:Noun+A3sg|lük:Ness→Noun+A3sg|lü:With→Adj  \n",
       "8   [gözcü:Noun] gözcü:Noun+A3sg                             \n",
       "9   [göz:Noun] göz:Noun+A3sg|cü:Agt→Noun+A3sg                \n",
       "10  [göz:Noun] göz:Noun+A3sg+den:Abl                         \n",
       "11  [Gözde:Noun,Prop] gözde:Noun+A3sg+n:P2sg                 \n",
       "12  [gözde:Noun] gözde:Noun+A3sg+n:P2sg                      \n",
       "13  [gözde:Adj] gözde:Adj|Zero→Noun+A3sg+n:P2sg              \n",
       "14  [düşmek:Verb] düş:Verb+tü:Past+A3sg                      \n",
       "15  [düş:Noun] düş:Noun+A3sg|Zero→Verb+tü:Past+A3sg          \n",
       "16  [.:Punc] .:Punc                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from zeyrek import MorphAnalyzer, rulebasedanalyzer\n",
    "import logging.config\n",
    "\n",
    "# Disables redundant error messages from zeyrek\n",
    "logging.config.dictConfig({\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': True,\n",
    "})\n",
    "\n",
    "analyzer = MorphAnalyzer()\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "def format_groups(groups):\n",
    "    # This function flattens the list of lists,\n",
    "    # removes redundant columns and, outputs a dataframe\n",
    "    out = functools.reduce(operator.iconcat, groups, [])\n",
    "    lines = []\n",
    "    for item in out:\n",
    "        item = item._asdict()\n",
    "        item.pop('morphemes')\n",
    "        lines.append(item)\n",
    "\n",
    "    return pd.DataFrame(lines)\n",
    "\n",
    "format_groups(analyzer.analyze(sentence))\n",
    "\n",
    "# This also explicitly returns tokenized sentences if you prefer:\n",
    "# analyzer._analyze_text(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480f795",
   "metadata": {},
   "source": [
    "Since some words can have different morphological explanations, every alternative is retrieved. From this, we can obtain lemmas and POS tags as you can see. However, simply using the first explanation for a word may not yield the correct result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa04064",
   "metadata": {},
   "source": [
    "We can also simply use `analyzer.lemmatize()` to lemmatize the words. Again, it returns all possible lemmas without disambiguation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfc4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gözleme', ['gözlemek', 'gözlem', 'Gözlem', 'gözleme']),\n",
       " ('gözleyen', ['gözlemek']),\n",
       " ('gözlüklü', ['gözlük', 'göz']),\n",
       " ('gözcü', ['gözcü', 'göz']),\n",
       " ('gözden', ['Gözde', 'göz', 'gözde']),\n",
       " ('düştü', ['düş', 'düşmek']),\n",
       " ('.', ['.'])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzer.lemmatize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73268e1d",
   "metadata": {},
   "source": [
    "### Dealing with Turkish characters<a id=\"tr-encoding\"></a>\n",
    "\n",
    "A past Turkish government's short-sightedness in the 80's is haunting programmers who work with Turkish texts to this day. To prevent some Turkish characters from deforming, we need to read and write files using the \"UTF-8\" encoding. However, this may not be enough if your data is not saved as \"UTF-8\" in the first place. Sometimes, you may realize that certain characters in your dataset itself is not properly represented. For example, instead of the word `kılıç`, you may see `kÄ±lÄ±Ã§`. This may suggest that your data is saved in \"Latin-1\" (also known as \"ISO 8859-1\"), which is the most common encoding in the Western world and the standard for many protocols. To fix it, you can specify the encoding of the string as \"latin-1\" (or \"ISO 8859-1\") and then decode it to \"UTF-8\" as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c417cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'kılıç'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "badly_encoded = \"kÄ±lÄ±Ã§\"\n",
    "encoding_fixed = badly_encoded.encode(\"latin-1\").decode(\"utf-8\")\n",
    "\n",
    "encoding_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4405d05",
   "metadata": {},
   "source": [
    "Hopefully, this will solve your problem. If not, you can also find the deformed characters and write a function that replaces all of those characters with the correct ones.\n",
    "\n",
    "Keep in mind that Turkish characters can also cause some packages or languages to raise an error. You may also see that the fonts used by some packages may not support these characters and plot `□` instead. Therefore, you may want to anglicize Turkish characters to prevent these. Here is a function that does that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95b433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'kilic'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Changes all Turkish characters with their simpler versions:\n",
    "def anglicize_turkish(text):\n",
    "    return text.translate(text.maketrans({\"Ğ\": \"G\", \"ğ\": \"g\", \"Ü\": \"U\", \"ü\": \"u\", \"Ş\": \"S\", \"ş\": \"s\", \"İ\": \"I\", \"ı\": \"i\", \"Ö\": \"O\", \"ö\": \"o\", \"Ç\": \"C\", \"ç\": \"c\"}))\n",
    "\n",
    "anglicize_turkish(\"kılıç\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed0e14",
   "metadata": {},
   "source": [
    "Another problem is the built-in functions that lowercase or uppercase your text may not work properly with Turkish text, even if your system locale is Turkish. For example, the lowercase version of `KILIÇ` is normally returned as `kiliç`, since \"I\" corresponds to \"i\" in the Western languages. Instead of dealing with locales (which may not solve your problem anyway), a simple solution is to manually lowercase/uppercase the problematic letters and then use the built-in function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5199926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built-in lowercase for KILIÇ: kiliç\n",
      "Custom lowercase for KILIÇ: kılıç\n",
      "Built-in uppercase for isim: ISIM\n",
      "Custom uppercase for isim: İSİM\n"
     ]
    }
   ],
   "source": [
    "def turkish_lowercase(text):\n",
    "    return text.translate(text.maketrans({\"I\": \"ı\"})).lower()\n",
    "\n",
    "def turkish_uppercase(text):\n",
    "    return text.translate(text.maketrans({\"i\": \"İ\"})).upper()\n",
    "\n",
    "print(\"Built-in lowercase for KILIÇ:\", \"KILIÇ\".lower())\n",
    "print(\"Custom lowercase for KILIÇ:\", turkish_lowercase(\"KILIÇ\"))\n",
    "print(\"Built-in uppercase for isim:\", \"isim\".upper())\n",
    "print(\"Custom uppercase for isim:\", turkish_uppercase(\"isim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c82cd",
   "metadata": {},
   "source": [
    "### Discussion Question\n",
    "\n",
    "* [HuggingFace](https://huggingface.co) is a platform where organizations and people share open weight deep learning models, datasets and more.<br><br> Do a quick search on HuggingFace for current representation of Turkish in NLP. (Note that this won't be representative of actual research.) <br> How recent is the most downloaded model? What datasets were used for training or finetuning in general?\n",
    "\n",
    "\n",
    "* What is the benefit of having NLP models specific to Turkish?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e16748",
   "metadata": {},
   "source": [
    "> Answers can be written here by double clicking and editing the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae568cb8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
